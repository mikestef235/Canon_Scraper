{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canon DLSR Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from more_itertools import unique_everseen\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "import time\n",
    "import csv\n",
    "import datetime as dt\n",
    "import warnings\n",
    "from fuzzywuzzy import fuzz\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting the driver and finding the query element\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://washingtondc.craigslist.org/nva\")\n",
    "search = driver.find_element_by_id(\"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputing the search keyword\n",
    "search.send_keys(\"Canon\")\n",
    "search.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the photo/video and electronics cateories\n",
    "driver.find_element_by_css_selector('input.catcheck.selectallcb').click()\n",
    "driver.find_element_by_css_selector(\".catcheck.multi_checkbox[id='cat_pha']\").click()\n",
    "driver.find_element_by_css_selector(\".catcheck.multi_checkbox[id='cat_ela']\").click()\n",
    "driver.find_element_by_css_selector(\"button.searchlink.linklike.changed_input.clickme\").click()\n",
    "\n",
    "#Bundling duplicates, requiring pictures, not including nearby searches\n",
    "driver.find_element_by_name(\"bundleDuplicates\").click()\n",
    "driver.find_element_by_name(\"hasPic\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputing the min and max price\n",
    "minP = driver.find_element_by_name(\"min_price\")\n",
    "minP.send_keys(500)\n",
    "time.sleep(1)\n",
    "maxP = driver.find_element_by_name(\"max_price\")\n",
    "maxP.send_keys(1500)\n",
    "maxP.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Eligible Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepping for beautiful soup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going through all content divs and finding the links\n",
    "nduplinks = []\n",
    "duplinks=[] \n",
    "\n",
    "#Finding the non duplicated results\n",
    "nonDups = soup.findAll('li', {'class': 'result-row'})\n",
    "for item in nonDups:\n",
    "    nduplinks.append(item.find('a').get('href'))\n",
    "    \n",
    "#Finding the duplicated results\n",
    "dups = soup.findAll('ul', {'class': 'duplicate-rows'})\n",
    "for item in dups:\n",
    "    duplinks.append(item.find('a').get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the duplicates, returning the links list\n",
    "links = [x for x in nduplinks if x not in duplinks]\n",
    "\n",
    "#Removing duplicate links due to pictures and titles both being the same\n",
    "links = list(unique_everseen(links))\n",
    "while \"#\" in links:\n",
    "    links.remove(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking into the item links\n",
    "count = 0\n",
    "dailyitems=[]\n",
    "itemDict = []\n",
    "for link in links:\n",
    "    #Clicking the item link\n",
    "    link = '\"'+link+'\"'\n",
    "    element = driver.find_element_by_xpath('//a[@href=%s]' %link)\n",
    "    driver.execute_script('arguments[0].click();', element)\n",
    "\n",
    "    #Getting the item html for beautiful soup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    #Getting the item price and title\n",
    "    price = soup.find('span', {'class': 'price'}).get_text()\n",
    "    title = soup.find('span', {'id': 'titletextonly'}).get_text()\n",
    "    \n",
    "    #Getting the item text\n",
    "    text = soup.find('section', {'id':'postingbody'}).get_text()\n",
    "    while \"\\n\" in text:\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "    while \"QR Code Link to This Post\" in text:\n",
    "        text = text.replace(\"QR Code Link to This Post\", \"\")\n",
    "    itemD = {\"Title\":title, \"Price\":price, \"Description\":text}\n",
    "\n",
    "    #Making a list of dictionaries\n",
    "    itemDict.append(itemD)\n",
    "\n",
    "    #Adding to a list\n",
    "    dailyitems.append([itemD['Title'], itemD['Price'], itemD['Description']])\n",
    "    count += 1\n",
    "    driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outputting the results array item by item\n",
    "driver.close()\n",
    "\n",
    "#Creating a dataframe of the items\n",
    "current_listings = pd.DataFrame(itemDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the dollar sign in current listings:\n",
    "if current_listings.Price.dtypes.name != 'int64':\n",
    "    current_listings['Price'] = pd.to_numeric(current_listings['Price'].str.replace('$', ''))\n",
    "\n",
    "#Outputting current listings\n",
    "current_listings = current_listings[['Title', 'Description', 'Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recording the Time the Script Finished Crawling\n",
    "time_ran = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Current Listings and Sold Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Current Listings from Yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the listings from the day before (Dataframe1)\n",
    "old_listings = pd.read_csv('C:/Users/mikes/Documents/Python Scripts/Canon_Scraper/Data_Tables/CanonCurrentListings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Yesterday's and Today's Listings With the Same Title and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = old_listings.merge(current_listings, on=['Title', 'Description'])\n",
    "common['Current_Price'] = common['Price']\n",
    "common.drop('Price', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider Cases Where the Description Has Been Changed Slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding today's listings not perfectly matched in yesterdays listings\n",
    "compDF = current_listings.merge(old_listings, indicator=True, how='outer', on=['Title', 'Description'])\n",
    "only_today = compDF[compDF['_merge'] == 'left_only']\n",
    "\n",
    "#Finding items in yesterdays listings not perfectly matched in todays listings\n",
    "only_old = compDF[compDF['_merge'] == 'right_only']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each item that is only in yesterdays listings, we need to compare the similarity of description to every item that is only in today's listing. If the fuzz ratio is greater than 85 (out of 100), we will consider the items a match and merge the old and new values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the matched listings between today and old only\n",
    "cols = ['Current_Price','Description','List_Date','List_Price','Title']\n",
    "matches = pd.DataFrame(columns=cols)\n",
    "\n",
    "#Performing the matching\n",
    "for old_index, old_row in only_old.iterrows():\n",
    "    for new_index, new_row in only_today.iterrows():\n",
    "        if fuzz.ratio(old_row.Description, new_row.Description) > 85:\n",
    "            #Add listing to matches dataframe\n",
    "            matches = matches.append({'Current_Price':new_row.Price, \n",
    "                            'Description':new_row.Description, \n",
    "                            'List_Date':old_row.List_Date,\n",
    "                            'List_Price':old_row.List_Price, \n",
    "                            'Title':new_row.Title}, ignore_index=True)\n",
    "            \n",
    "            #Remove item from old_only\n",
    "            only_old = only_old[~(only_old.Description == old_row.Description)]\n",
    "            \n",
    "            #Remove item from new_only\n",
    "            only_today = only_today[~(only_today.Description == new_row.Description)]\n",
    "         \n",
    "            #Break out of the inner looping\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the matches to the common listings\n",
    "common = common.append(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding New Items to Current Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the list day to new items dataframe\n",
    "new_items = only_today[['Title', 'Description', 'Price']]\n",
    "\n",
    "new_items['List_Date'] = time_ran.date()\n",
    "\n",
    "#Renaming price to be List Price \n",
    "new_items.rename(columns = {'Price':'List_Price'}, inplace=True)\n",
    "\n",
    "#Reflect the Current Price to be equal to the list price\n",
    "new_items['Current_Price'] = new_items['List_Price']\n",
    "\n",
    "#Adding new items to the common listings\n",
    "today_listings = common.append(new_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the listings to csv to be used tomorrow\n",
    "today_listings.to_csv('C:/Users/mikes/Documents/Python Scripts/Canon_Scraper/Data_Tables/CanonCurrentListings.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addding Items Sold Today to Sold Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_today = only_old\n",
    "\n",
    "#Updating the historical table of sold items\n",
    "if sold_today.shape[0] != 0:\n",
    "    sold_today.drop(['_merge', 'Price'], axis=1, inplace=True)\n",
    "    sold_today.rename(columns={'Current_Price':'Sell_Price'}, inplace=True)\n",
    "    sold_today['Sell_Date'] = time_ran.date()\n",
    "    \n",
    "    #Loading the sold items csv\n",
    "    sold_items = pd.read_csv('C:/Users/mikes/Documents/Python Scripts/Canon_Scraper/Data_Tables/SoldItems.csv')\n",
    "    \n",
    "    #Adding items sold today to sold_items\n",
    "    sold_items = sold_items.append(sold_today)\n",
    "    \n",
    "    #Writing to the sold items csv\n",
    "    sold_items.to_csv('C:/Users/mikes/Documents/Python Scripts/Canon_Scraper/Data_Tables/SoldItems.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
