{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canon DLSR Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from more_itertools import unique_everseen\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "import time\n",
    "import csv\n",
    "import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting the driver and finding the query element\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://washingtondc.craigslist.org/nva\")\n",
    "search = driver.find_element_by_id(\"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputing the search keyword\n",
    "search.send_keys(\"Canon\")\n",
    "search.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the photo/video and electronics cateories\n",
    "driver.find_element_by_css_selector('input.catcheck.selectallcb').click()\n",
    "driver.find_element_by_css_selector(\".catcheck.multi_checkbox[id='cat_pha']\").click()\n",
    "driver.find_element_by_css_selector(\".catcheck.multi_checkbox[id='cat_ela']\").click()\n",
    "driver.find_element_by_css_selector(\"button.searchlink.linklike.changed_input.clickme\").click()\n",
    "\n",
    "#Bundling duplicates, requiring pictures, not including nearby searches\n",
    "driver.find_element_by_name(\"bundleDuplicates\").click()\n",
    "driver.find_element_by_name(\"hasPic\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputing the min and max price\n",
    "minP = driver.find_element_by_name(\"min_price\")\n",
    "minP.send_keys(500)\n",
    "time.sleep(1)\n",
    "maxP = driver.find_element_by_name(\"max_price\")\n",
    "maxP.send_keys(1500)\n",
    "maxP.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Eligible Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepping for beautiful soup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going through all content divs and finding the links\n",
    "nduplinks = []\n",
    "duplinks=[] \n",
    "#Finding the non duplicated results\n",
    "nonDups = soup.findAll('li', {'class': 'result-row'})\n",
    "for item in nonDups:\n",
    "    nduplinks.append(item.find('a').get('href'))\n",
    "#Finding the duplicated results\n",
    "dups = soup.findAll('ul', {'class': 'duplicate-rows'})\n",
    "for item in dups:\n",
    "    duplinks.append(item.find('a').get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the duplicates, returning the links list\n",
    "links = [x for x in nduplinks if x not in duplinks]\n",
    "\n",
    "#Removing duplicate links due to pictures and titles both being the same\n",
    "links = list(unique_everseen(links))\n",
    "while \"#\" in links:\n",
    "    links.remove(\"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR TEST PURPOSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking into the item links\n",
    "count = 0\n",
    "dailyitems=[]\n",
    "itemDict = []\n",
    "for link in links:\n",
    "    #Clicking the item link\n",
    "    link = '\"'+link+'\"'\n",
    "    element = driver.find_element_by_xpath('//a[@href=%s]' %link)\n",
    "    driver.execute_script('arguments[0].click();', element)\n",
    "\n",
    "    #Getting the item html for beautiful soup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    #Getting the item price and title\n",
    "    price = soup.find('span', {'class': 'price'}).get_text()\n",
    "    title = soup.find('span', {'id': 'titletextonly'}).get_text()\n",
    "    #Getting the item text\n",
    "    text = soup.find('section', {'id':'postingbody'}).get_text()\n",
    "    while \"\\n\" in text:\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "    while \"QR Code Link to This Post\" in text:\n",
    "        text = text.replace(\"QR Code Link to This Post\", \"\")\n",
    "    itemD = {\"Title\":title, \"Price\":price, \"Description\":text}\n",
    "\n",
    "    #Making a list of dictionaries\n",
    "    itemDict.append(itemD)\n",
    "\n",
    "    #Adding to a list\n",
    "    dailyitems.append([itemD['Title'], itemD['Price'], itemD['Description']])\n",
    "    count += 1\n",
    "    driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outputting the results array item by item\n",
    "driver.close()\n",
    "#pprint(dailyitems)\n",
    "df = pd.DataFrame(itemDict)\n",
    "current_listings = df\n",
    "#To Generate the first days csv\n",
    "#current_listings.to_csv('CanonCurrentListings.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the dollar sign in current listings:\n",
    "if current_listings.Price.dtypes.name != 'int64':\n",
    "    current_listings['Price'] = pd.to_numeric(current_listings['Price'].str.replace('$', ''))\n",
    "\n",
    "#Outputting current listings\n",
    "current_listings = current_listings[['Title', 'Description', 'Price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recording the Time the Script Finished Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ran = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Comparisons "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Past 'current' Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the listings from the day before (Dataframe1)\n",
    "old_listings = pd.read_csv('C:/Users/mikes/Documents/Python Scripts/Canon_Scraper/Data_Tables/CanonCurrentListings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Common and New Listings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If old listing title is not in current listing, assume the item has sold\n",
    "common = old_listings.merge(current_listings, on=['Title', 'Description'])\n",
    "common['Current_Price'] = common['Price']\n",
    "common.drop('Price', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the items not already being tracked\n",
    "new_items=(current_listings[(~current_listings.Title.isin(common.Title))])\n",
    "\n",
    "#Adding the list day to new items dataframe\n",
    "new_items['List_Date'] = time_ran.date()\n",
    "\n",
    "#Renaming price to be List Price \n",
    "new_items.rename(columns = {'Price':'List_Price'}, inplace=True)\n",
    "\n",
    "#Reflect the Current Price to be equal to the list price\n",
    "new_items['Current_Price'] = new_items['List_Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding new items to the common listings\n",
    "today_listings = common.append(new_items)\n",
    "\n",
    "#Writing the listings to csv to be used tomorrow\n",
    "today_listings.to_csv('C:/Users/mikes/Documents/Python Scripts/Canon_Scraper/Data_Tables/CanonCurrentListings.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Sold Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = old_listings.merge(current_listings, indicator=True, how='outer', left_on=['Title', 'Description'], right_on=['Title', 'Description'])\n",
    "sold_today = merged[merged['_merge'] == 'left_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sold_today.shape[0] != 0:\n",
    "    sold_today.drop(['_merge'], axis=1, inplace=True)\n",
    "    sold_today.rename(columns={'Price':'Sell_Price'}, inplace=True)\n",
    "    sold_today['Sell_Date'] = time_ran.date()\n",
    "    \n",
    "    #Loading the sold items csv\n",
    "    sold_items = pd.read_csv('C:/Users/mikes/Documents/Python Scripts/Canon_Scraper/Data_Tables/SoldItems.csv')\n",
    "    \n",
    "    #Adding items sold today to sold_items\n",
    "    sold_items = sold_items.append(sold_today)\n",
    "    \n",
    "    #Writing to the sold items csv\n",
    "    sold_items.to_csv('C:/Users/mikes/Documents/Python Scripts/Canon_Scraper/Data_Tables/SoldItems.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
